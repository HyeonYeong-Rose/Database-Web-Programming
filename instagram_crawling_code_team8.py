# -*- coding: utf-8 -*-
"""instagram_crawling_code_Team8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P6gmExITLx8Gyrtg-YbAkGkaG-dK7GRc
"""

import pandas as pd
import numpy as np

import selenium
from selenium import webdriver

from bs4 import BeautifulSoup as bs
from urllib.request import urlopen, Request
from urllib.parse import quote_plus
import random
import time
from tqdm import tqdm_notebook
import warnings
warnings.filterwarnings('ignore')

def instagram_login(id, pw):
    driver.get(url)
    driver.implicitly_wait(5)
    driver.find_element_by_name('username').send_keys(id)    # id 입력
    elem_pw = driver.find_element_by_name('password')    # pw 입력
    elem_pw.send_keys(pw)
    elem_pw.submit()
    
    driver.implicitly_wait(5)    # 파싱될 때까지 5초 기다림 (미리 완료되면 waiting 종료됨)
    driver.find_element_by_class_name('cmbtv').click()    # 비밀번호 저장하지 않음
    
    driver.implicitly_wait(5)
    driver.find_element_by_xpath('/html/body/div[4]/div/div/div/div[3]/button[2]').click()    # 알림설정 무시
    
    
def main_search(keyword):
    search = driver.find_element_by_xpath('//*[@id="react-root"]/section/nav/div[2]/div/div/div[2]/input')
    search.clear()
    search.send_keys(keyword)
    search_list1 = driver.find_element_by_xpath('//*[@id="react-root"]/section/nav/div[2]/div/div/div[2]/div[3]/div/div[2]/div/a')
    search_list1.click()
    
    
    #접속주소 설정
url = 'http://www.instagram.com'
path = 'C:\\Users\\Seo\\Desktop\\DB_Web_Programming\\chromedriver.exe'
driver = webdriver.Chrome(path)

instagram_login("userID", "password")

import pandas as pd
df=pd.read_csv("C:\\Users\\Seo\\Desktop\\서울시 자치구별 동 데이터다.csv", encoding="euc-kr")
df
df.head()

seoul_list = []

for i in range (0,240):
    #print(df.values[i][0])
    
    seoul_list.append(df.values[i][0])
    
#print(seoul_list)
def search(i):
    keyword = seoul_list[i] + '카페'
    search = driver.find_element_by_xpath('//*[@id="react-root"]/section/nav/div[2]/div/div/div[2]/input')
    search.clear()
    search.send_keys(keyword)

date_list = []
time_list = []
loc_list = []

# 111 ~ 
page = 1
for i in range(0,240):  
    
    if page >= 2:
        time.sleep(4)
    search(i)
    print(seoul_list[i])
    print('---------------------------------------')
    time.sleep(4)
    
    if page >= 2:
        time.sleep(5)
    #검색결과 중에 첫번째 클릭
    search_list1 = driver.find_element_by_xpath('//*[@id="react-root"]/section/nav/div[2]/div/div/div[2]/div[3]/div/div[2]/div/div[1]/a/div')
    search_list1.click()
    #'//*[@id="react-root"]/section/nav/div[2]/div/div/div[2]/div[3]/div/div[2]/div/div[1]/a/div'
    if page >= 2:
        time.sleep(6)
    #검색결과 중 게시글 첫번째꺼 클릭
    time.sleep(5)
    search_first= driver.find_element_by_xpath('//*[@id="react-root"]/section/main/article/div/div/div/div/div/a/div/div[2]')
    search_first.click()
    time.sleep(2)
    
    #게시글 내부 데이터 가져오기

    random_time =[0.40, 0.45, 0.5, 0.38, 0.72, 0.6, 0.7, 0.55]

    SCROLL_PAUSE_TIME = random_time[random.randrange(0, 8)]
    print(SCROLL_PAUSE_TIME)
    if page >= 2:
        time.sleep(4)
        
    count =0
    page = 0
    while count <= 200:
        a = 0
   #위치 가져오기
        
        try:
            search_location=driver.find_element_by_xpath('/html/body/div[5]/div[2]/div/article/header/div[2]/div[2]/div/a').text
            print(search_location)
            loc_list.append(search_location)
            
            search_date=driver.find_element_by_xpath('/html/body/div[5]/div[2]/div/article/div[3]/div[2]/a/time').get_attribute('datetime')
   
            result_date=search_date[0:10]
            print(result_date)
            date_list.append(result_date)
    
            result_time=search_date[11:16]
            print(result_time)
            time_list.append(result_time)
        except Exception:
            print("There's no location info.")
            print("No date_time")
            if page == 0:
                a = a+1
                go_next = driver.find_element_by_xpath('/html/body/div[5]/div[1]/div/div/a')
                go_next.click()
            else : 
                go_next = driver.find_element_by_xpath('/html/body/div[5]/div[1]/div/div/a[2]')
                go_next.click()
            #loc_list.append('NULL')
            #SCROLL_PAUSE_TIME = 0.05
            pass
          
            
    #다음 게시물 클릭
        try:
            if page == 0:
                if a == 1:
                    go_next = driver.find_element_by_xpath('/html/body/div[5]/div[1]/div/div/a[2]')
                    go_next.click()
                else:
                    go_next = driver.find_element_by_xpath('/html/body/div[5]/div[1]/div/div/a')
                    go_next.click()
            else:
                go_next = driver.find_element_by_xpath('/html/body/div[5]/div[1]/div/div/a[2]')
                go_next.click()
            page += 1       
        except Exception:
            print('pass')
            pass
                
        print('{0}Posts are successfully collected.'.format(len(date_list)))
        time.sleep(SCROLL_PAUSE_TIME)
        
        count +=1
    time.sleep(3)    
    i += 1
    
   
    go_next = driver.find_element_by_xpath('/html/body/div[5]/div[3]/button')
    go_next.click()
    
    
    time.sleep(3) 
    page += 1

len(loc_list)

print(loc_list)

merge=list(zip(date_list, time_list,loc_list))
print(merge)

import pandas as pd

data = pd.DataFrame(merge)

data.to_csv('Insta_data.csv')



